"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–π –∏ –∫–æ—Å–æ—É–≥–æ–ª—å–Ω—ã–π –±–∞–∑–∏—Å.

–ö–æ—Å–æ—É–≥–æ–ª—å–Ω—ã–π –±–∞–∑–∏—Å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–æ —Ñ–æ—Ä–º—É–ª–µ:
    v_c = Œ±¬∑v_p + Œ≤¬∑u_c
–≥–¥–µ:
    Œ± = 1/N –¥–ª—è N‚â•2 (N = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ siblings)
    Œ± = decay_factor –¥–ª—è N=1 (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞)
    Œ≤ = sqrt(1 - Œ±¬≤)
"""

from __future__ import annotations
import os
from pathlib import Path
from typing import Callable, Dict, List, Literal, Optional, Tuple, Set
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.metrics.pairwise import euclidean_distances, cosine_distances

# ============================================================================
# –¢–ò–ü–´ –ò –ö–û–ù–°–¢–ê–ù–¢–´
# ============================================================================

DistanceMetric = Literal[
    "euclidean_orthogonal",
    "cosine_orthogonal",
    "euclidean_oblique",
    "cosine_oblique"
]

ComparisonScope = Literal["direct", "all"]

DISTANCE_METRIC_LABELS: Dict[DistanceMetric, str] = {
    "euclidean_orthogonal": "–ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–π –±–∞–∑–∏—Å)",
    "cosine_orthogonal": "–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–π –±–∞–∑–∏—Å)",
    "euclidean_oblique": "–ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–∫–æ—Å–æ—É–≥–æ–ª—å–Ω—ã–π –±–∞–∑–∏—Å)",
    "cosine_oblique": "–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–∫–æ—Å–æ—É–≥–æ–ª—å–Ω—ã–π –±–∞–∑–∏—Å)",
}

SCOPE_LABELS: Dict[ComparisonScope, str] = {
    "direct": "–¢–æ–ª—å–∫–æ –ø—Ä—è–º—ã–µ —É—á–µ–Ω–∏–∫–∏",
    "all": "–í—Å—è –ª–∏–Ω–∏—è (–≤–∫–ª—é—á–∞—è –ø–æ—Ç–æ–º–∫–æ–≤)",
}

SILHOUETTE_COLORS = [
    "#FF8C42", "#FFD166", "#F77F00", "#FCBF49", "#EF476F",
    "#06D6A0", "#118AB2", "#073B4C", "#E07A5F", "#81B29A",
]

# ============================================================================
# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –†–ê–ë–û–¢–´ –° –ò–ï–†–ê–†–•–ò–ï–ô
# ============================================================================

def get_code_depth(code: str) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª—É–±–∏–Ω—É —É–∑–ª–∞ –≤ –∏–µ—Ä–∞—Ä—Ö–∏–∏."""
    if not code:
        return 0
    return code.count('.') + 1


def get_parent_code(code: str) -> Optional[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥ —Ä–æ–¥–∏—Ç–µ–ª—è. '1.2.3' -> '1.2'."""
    if '.' not in code:
        return None
    return code.rsplit('.', 1)[0]


def get_ancestor_codes(code: str) -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—Ä–µ–¥–∫–æ–≤ (–≤–∫–ª—é—á–∞—è —Å–∞–º –∫–æ–¥)."""
    ancestors = []
    current = code
    while current:
        ancestors.insert(0, current)
        current = get_parent_code(current)
    return ancestors


def is_descendant_of(code: str, ancestor: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ code –ø–æ—Ç–æ–º–∫–æ–º ancestor."""
    if code == ancestor:
        return True
    return code.startswith(ancestor + '.')


def filter_columns_by_nodes(
    columns: List[str],
    selected_nodes: Optional[List[str]] = None
) -> List[str]:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —É–∑–ª–∞–º –∏–µ—Ä–∞—Ä—Ö–∏–∏."""
    if selected_nodes is None or len(selected_nodes) == 0:
        return columns
    filtered = []
    for col in columns:
        for node in selected_nodes:
            if is_descendant_of(col, node):
                filtered.append(col)
                break
    return filtered


def get_nodes_at_level(columns: List[str], level: int) -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —É–∑–ª—ã –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è."""
    return sorted(set(col for col in columns if get_code_depth(col) == level))


def get_selectable_nodes(columns: List[str], max_level: int = 3) -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —É–∑–ª—ã –æ—Ç —É—Ä–æ–≤–Ω—è 1 –¥–æ max_level."""
    result = []
    for level in range(1, max_level + 1):
        result.extend(get_nodes_at_level(columns, level))
    return sorted(result)


def get_sibling_count(code: str, all_codes: List[str]) -> int:
    """
    –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ siblings (—É–∑–ª–æ–≤ —Å —Ç–µ–º –∂–µ —Ä–æ–¥–∏—Ç–µ–ª–µ–º).
    –í–∫–ª—é—á–∞–µ—Ç —Å–∞–º —É–∑–µ–ª code.
    """
    parent = get_parent_code(code)
    if parent is None:
        # –ö–æ—Ä–Ω–µ–≤–æ–π —É—Ä–æ–≤–µ–Ω—å: —Å—á–∏—Ç–∞–µ–º –≤—Å–µ —É–∑–ª—ã –±–µ–∑ —Ç–æ—á–µ–∫
        return sum(1 for c in all_codes if '.' not in c)
    # –°—á–∏—Ç–∞–µ–º –¥–µ—Ç–µ–π —Ç–æ–≥–æ –∂–µ —Ä–æ–¥–∏—Ç–µ–ª—è
    return sum(1 for c in all_codes if get_parent_code(c) == parent)


# ============================================================================
# –ö–û–°–û–£–ì–û–õ–¨–ù–´–ô –ë–ê–ó–ò–° (–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø)
# ============================================================================

def build_oblique_basis_matrix(
    feature_columns: List[str],
    decay_factor: float = 0.6
) -> np.ndarray:
    """
    –°—Ç—Ä–æ–∏—Ç –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Å–æ—É–≥–æ–ª—å–Ω–æ–≥–æ –±–∞–∑–∏—Å–∞ B.
    
    –°—Ç—Ä–æ–∫–∏ B ‚Äî —ç—Ç–æ –±–∞–∑–∏—Å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã v_i –≤ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö.
    
    –§–æ—Ä–º—É–ª–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞ c —Å —Ä–æ–¥–∏—Ç–µ–ª–µ–º p:
        v_c = Œ±¬∑v_p + Œ≤¬∑u_c
    –≥–¥–µ:
        - N = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ siblings (–≤–∫–ª—é—á–∞—è —Å–∞–º —É–∑–µ–ª)
        - Œ± = decay_factor, –µ—Å–ª–∏ N=1 (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–ª–∞–ø—Å v_c=v_p)
        - Œ± = 1/N, –µ—Å–ª–∏ N‚â•2 (—Å—Ç—Ä–æ–≥–∞—è —Ñ–æ—Ä–º—É–ª–∞ –∏–∑ –º–µ—Ç–æ–¥–∞)
        - Œ≤ = sqrt(1 - Œ±¬≤)
        - u_c ‚Äî –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ (—Ç–µ–∫—É—â–∞—è —Å—Ç—Ä–æ–∫–∞ B[i])
    
    Args:
        feature_columns: —Å–ø–∏—Å–æ–∫ –∫–æ–¥–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        decay_factor: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è –¥–ª—è N=1 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 0.5-0.7)
        
    Returns:
        –ú–∞—Ç—Ä–∏—Ü–∞ B —Ä–∞–∑–º–µ—Ä–∞ (n_features, n_features)
    """
    n = len(feature_columns)
    code_to_idx = {c: i for i, c in enumerate(feature_columns)}
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ i ‚Äî —ç—Ç–æ –µ–¥–∏–Ω–∏—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä u_i
    B = np.eye(n, dtype=np.float64)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏: –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é –≥–ª—É–±–∏–Ω—ã
    # (—Ä–æ–¥–∏—Ç–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è —Ä–∞–Ω—å—à–µ –¥–µ—Ç–µ–π)
    processing_order = sorted(range(n), key=lambda i: feature_columns[i].count('.'))
    
    # –°—á—ë—Ç—á–∏–∫–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    n_strict = 0   # –£–∑–ª—ã —Å N‚â•2 (—Å—Ç—Ä–æ–≥–∞—è —Ñ–æ—Ä–º—É–ª–∞)
    n_decay = 0    # –£–∑–ª—ã —Å N=1 (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è decay_factor)
    n_root = 0     # –£–∑–ª—ã –±–µ–∑ —Ä–æ–¥–∏—Ç–µ–ª—è –≤ –Ω–∞–±–æ—Ä–µ
    
    for i in processing_order:
        code = feature_columns[i]
        parent_code = get_parent_code(code)
        
        # –ï—Å–ª–∏ —Ä–æ–¥–∏—Ç–µ–ª—è –Ω–µ—Ç –≤ –Ω–∞–±–æ—Ä–µ, —É–∑–µ–ª –æ—Å—Ç–∞—ë—Ç—Å—è –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–º
        if parent_code is None or parent_code not in code_to_idx:
            n_root += 1
            continue
        
        parent_idx = code_to_idx[parent_code]
        N = get_sibling_count(code, feature_columns)
        
        # –í—ã–±–æ—Ä Œ± –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç N
        if N == 1:
            alpha = decay_factor
            n_decay += 1
        else:
            alpha = 1.0 / N
            n_strict += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
        alpha = min(alpha, 0.9999)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Œ≤
        beta = np.sqrt(1.0 - alpha * alpha)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑–∏—Å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞:
        # v_c = Œ±¬∑v_p + Œ≤¬∑u_c
        # B[parent_idx, :] ‚Äî —ç—Ç–æ —É–∂–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π v_p
        # B[i, :] ‚Äî —ç—Ç–æ —Ç–µ–∫—É—â–∏–π u_c (–±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω)
        B[i, :] = alpha * B[parent_idx, :] + beta * B[i, :]
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥
    print(f"  [–ö–æ—Å–æ—É–≥–æ–ª—å–Ω—ã–π –±–∞–∑–∏—Å] –ü–æ—Å—Ç—Ä–æ–µ–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ {n}√ó{n}")
    print(f"    ‚Ä¢ –£–∑–ª—ã —Å N‚â•2 (Œ±=1/N): {n_strict}")
    print(f"    ‚Ä¢ –£–∑–ª—ã —Å N=1 (Œ±={decay_factor:.2f}): {n_decay}")
    print(f"    ‚Ä¢ –ö–æ—Ä–Ω–µ–≤—ã–µ —É–∑–ª—ã: {n_root}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–∞ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏
    if n > 1:
        cond = np.linalg.cond(B)
        if cond > 100:
            print(f"    ‚ö†Ô∏è  –ß–∏—Å–ª–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏: {cond:.1f} (–≤–æ–∑–º–æ–∂–Ω–∞ –∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å)")
    
    return B


def apply_oblique_basis_transform(
    X: np.ndarray,
    feature_columns: List[str],
    decay_factor: float = 0.6
) -> np.ndarray:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ—Å–æ—É–≥–æ–ª—å–Ω–æ–µ –±–∞–∑–∏—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫ –¥–∞–Ω–Ω—ã–º.
    
    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: V = X @ B
    –≥–¥–µ B ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–æ—É–≥–æ–ª—å–Ω–æ–≥–æ –±–∞–∑–∏—Å–∞.
    
    Args:
        X: –º–∞—Ç—Ä–∏—Ü–∞ –¥–∞–Ω–Ω—ã—Ö (n_samples, n_features)
        feature_columns: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω!)
        decay_factor: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∑–∞—Ç—É—Ö–∞–Ω–∏—è –¥–ª—è N=1
        
    Returns:
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ V (n_samples, n_features)
    """
    B = build_oblique_basis_matrix(feature_columns, decay_factor)
    # –ü–†–ê–í–ò–õ–¨–ù–û–ï —É–º–Ω–æ–∂–µ–Ω–∏–µ (–±–µ–∑ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è)
    return X @ B


# ============================================================================
# –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ê–¢–†–ò–¶–´ –†–ê–°–°–¢–û–Ø–ù–ò–ô
# ============================================================================

def compute_distance_matrix(
    data: np.ndarray,
    feature_columns: List[str],
    metric: DistanceMetric,
    decay_factor: float = 0.5
) -> np.ndarray:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –ø–æ–ø–∞—Ä–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π.
    
    Args:
        data: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (n_samples, n_features)
        feature_columns: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        metric: –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        decay_factor: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –∫–æ—Å–æ—É–≥–æ–ª—å–Ω–æ–≥–æ –±–∞–∑–∏—Å–∞
        
    Returns:
        –ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π (n_samples, n_samples)
    """
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—Å–æ—É–≥–æ–ª—å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if metric in ["euclidean_oblique", "cosine_oblique"]:
        data = apply_oblique_basis_transform(data, feature_columns, decay_factor)
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    if metric in ["euclidean_orthogonal", "euclidean_oblique"]:
        return euclidean_distances(data)
    else:  # cosine
        return cosine_distances(data)


# ============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================================

def load_scores_from_folder(
    folder_path: str = "basic_scores",
    specific_files: Optional[List[str]] = None
) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV-—Ñ–∞–π–ª—ã —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –∏–∑ –ø–∞–ø–∫–∏."""
    base = Path(folder_path).expanduser().resolve()
    
    if specific_files:
        files = [base / f for f in specific_files if (base / f).exists()]
    else:
        files = sorted(base.glob("*.csv"))
    
    if not files:
        raise FileNotFoundError(f"CSV-—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {base}")
    
    frames: List[pd.DataFrame] = []
    for file in files:
        try:
            frame = pd.read_csv(file)
            if 'Code' not in frame.columns:
                raise KeyError(f"{file.name}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'Code'")
            frames.append(frame)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file}: {e}")
            continue
    
    if not frames:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
    
    scores = pd.concat(frames, ignore_index=True)
    scores = scores.dropna(subset=['Code'])
    scores['Code'] = scores['Code'].astype(str).str.strip()
    scores = scores[scores['Code'].str.len() > 0]
    scores = scores.drop_duplicates(subset='Code', keep='first')
    
    feature_columns = [c for c in scores.columns if c != 'Code']
    scores[feature_columns] = scores[feature_columns].apply(pd.to_numeric, errors='coerce')
    scores[feature_columns] = scores[feature_columns].fillna(0.0)
    
    return scores


def get_feature_columns(scores: pd.DataFrame) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
    return [c for c in scores.columns if c != 'Code']


# ============================================================================
# –°–ë–û–† –î–ê–ù–ù–´–• –î–õ–Ø –ù–ê–£–ß–ù–û–ì–û –†–£–ö–û–í–û–î–ò–¢–ï–õ–Ø
# ============================================================================

def gather_school_dataset(
    df: pd.DataFrame,
    index: Dict[str, Set[int]],
    root: str,
    scores: pd.DataFrame,
    scope: ComparisonScope,
    lineage_func: Callable,
    rows_for_func: Callable,
    author_column: str = "candidate.name"
) -> Tuple[pd.DataFrame, pd.DataFrame, int]:
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –Ω–∞—É—á–Ω–æ–≥–æ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è."""
    if scope == "direct":
        subset = rows_for_func(df, index, root)
    elif scope == "all":
        subset = lineage_func(df, index, root)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π scope: {scope}")
    
    if subset is None or subset.empty:
        empty = pd.DataFrame(columns=['Code', 'school', author_column])
        return empty, empty, 0
    
    if 'Code' not in subset.columns:
        raise KeyError("Code")
    
    cols_to_keep = ['Code']
    if author_column in subset.columns:
        cols_to_keep.append(author_column)
    
    working = subset[cols_to_keep].copy()
    working['Code'] = working['Code'].astype(str).str.strip()
    working = working[working['Code'].str.len() > 0]
    working = working.drop_duplicates(subset='Code')
    
    if working.empty:
        empty = pd.DataFrame(columns=['Code', 'school', author_column])
        return empty, empty, 0
    
    codes = working['Code'].tolist()
    total_count = len(codes)
    
    scores_copy = scores.copy()
    scores_copy['Code'] = scores_copy['Code'].astype(str).str.strip()
    matched_scores = scores_copy[scores_copy['Code'].isin(codes)].copy()
    
    if matched_scores.empty:
        missing_info = working.copy()
        missing_info['school'] = root
        empty = pd.DataFrame(columns=list(scores.columns) + ['school', author_column])
        return empty, missing_info, total_count
    
    matched_scores['school'] = root
    
    if author_column in working.columns:
        matched_scores = matched_scores.merge(
            working[['Code', author_column]],
            on='Code',
            how='left'
        )
    else:
        matched_scores[author_column] = None
    
    found_codes = set(matched_scores['Code'].tolist())
    missing_codes = [c for c in codes if c not in found_codes]
    
    if missing_codes:
        missing_info = working[working['Code'].isin(missing_codes)].copy()
        missing_info['school'] = root
    else:
        missing_info = pd.DataFrame(columns=['Code', 'school', author_column])
    
    return matched_scores, missing_info, total_count


# ============================================================================
# –°–ò–õ–£–≠–¢–ù–´–ô –ê–ù–ê–õ–ò–ó
# ============================================================================

def compute_silhouette_analysis(
    datasets: Dict[str, pd.DataFrame],
    feature_columns: List[str],
    metric: DistanceMetric,
    selected_nodes: Optional[List[str]] = None,
    decay_factor: float = 0.5
) -> Tuple[float, np.ndarray, np.ndarray, List[str], List[str]]:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∏–ª—É—ç—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —à–∫–æ–ª."""
    used_columns = filter_columns_by_nodes(feature_columns, selected_nodes)
    if not used_columns:
        raise ValueError("–ù–µ –æ—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
    
    all_data = []
    all_labels = []
    school_order = []
    
    for school_name, dataset in datasets.items():
        if dataset.empty:
            continue
        
        available_cols = [c for c in used_columns if c in dataset.columns]
        if not available_cols:
            continue
        
        school_data = dataset[available_cols].fillna(0.0).values
        if school_data.shape[0] == 0:
            continue
        
        all_data.append(school_data)
        all_labels.extend([len(school_order)] * school_data.shape[0])
        school_order.append(school_name)
    
    if len(school_order) < 2:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —à–∫–æ–ª (–º–∏–Ω–∏–º—É–º 2)")
    
    X = np.vstack(all_data)
    labels = np.array(all_labels)
    
    if X.shape[0] < 2:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    distance_matrix = compute_distance_matrix(X, used_columns, metric, decay_factor)
    
    try:
        overall_score = silhouette_score(distance_matrix, labels, metric='precomputed')
        sample_scores = silhouette_samples(distance_matrix, labels, metric='precomputed')
    except Exception as e:
        raise ValueError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Å–∏–ª—É—ç—Ç–∞: {e}")
    
    return overall_score, sample_scores, labels, school_order, used_columns


# ============================================================================
# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
# ============================================================================

def create_silhouette_plot(
    sample_scores: np.ndarray,
    labels: np.ndarray,
    school_order: List[str],
    overall_score: float,
    metric_label: str
) -> plt.Figure:
    """–°–æ–∑–¥–∞—ë—Ç —Å–∏–ª—É—ç—Ç–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫."""
    n_schools = len(school_order)
    fig, ax = plt.subplots(figsize=(10, max(6, n_schools * 1.5)))
    
    y_lower = 10
    
    if n_schools <= len(SILHOUETTE_COLORS):
        colors = SILHOUETTE_COLORS[:n_schools]
    else:
        colors = SILHOUETTE_COLORS * ((n_schools // len(SILHOUETTE_COLORS)) + 1)
        colors = colors[:n_schools]
    
    for idx, school in enumerate(school_order):
        mask = labels == idx
        cluster_scores = sample_scores[mask]
        
        if cluster_scores.size == 0:
            continue
        
        cluster_scores = np.sort(cluster_scores)
        size = cluster_scores.size
        y_upper = y_lower + size
        
        ax.fill_betweenx(
            np.arange(y_lower, y_upper),
            0,
            cluster_scores,
            facecolor=colors[idx],
            edgecolor=colors[idx],
            alpha=0.85
        )
        
        ax.text(
            -0.05,
            y_lower + size / 2,
            f"{school}\n(n={size})",
            fontsize=10,
            va='center',
            ha='right',
            fontweight='medium'
        )
        
        y_lower = y_upper + 10
    
    ax.axvline(
        x=overall_score,
        color='#2D3436',
        linestyle='--',
        linewidth=2,
        label=f'–°—Ä–µ–¥–Ω–µ–µ = {overall_score:.3f}'
    )
    
    ax.set_xlim(-1, 1)
    ax.set_xlabel("–°–∏–ª—É—ç—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç", fontsize=12)
    ax.set_ylabel("–®–∫–æ–ª—ã", fontsize=12)
    ax.set_title(f"–°–∏–ª—É—ç—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ({metric_label})", fontsize=14, fontweight='bold')
    ax.set_yticks([])
    ax.legend(loc='lower right', fontsize=10)
    ax.grid(axis='x', linestyle='--', alpha=0.3)
    
    ax.axvspan(-1, -0.25, alpha=0.08, color='#e74c3c')
    ax.axvspan(-0.25, 0.25, alpha=0.08, color='#f39c12')
    ax.axvspan(0.25, 0.5, alpha=0.08, color='#27ae60')
    ax.axvspan(0.5, 1, alpha=0.08, color='#16a085')
    
    fig.tight_layout()
    return fig


def create_comparison_summary(
    datasets: Dict[str, pd.DataFrame],
    feature_columns: List[str],
    school_order: List[str]
) -> pd.DataFrame:
    """–°–æ–∑–¥–∞—ë—Ç —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É."""
    summary_data = []
    
    for school in school_order:
        if school not in datasets:
            continue
        data = datasets[school]
        if data.empty:
            continue
        
        available_cols = [c for c in feature_columns if c in data.columns]
        numeric_data = data[available_cols].fillna(0.0)
        
        summary_data.append({
            '–®–∫–æ–ª–∞': school,
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': len(data),
            '–°—Ä–µ–¥–Ω—è—è —Å—É–º–º–∞': numeric_data.sum(axis=1).mean(),
            '–ú–µ–¥–∏–∞–Ω–∞ —Å—É–º–º—ã': numeric_data.sum(axis=1).median(),
            '–°—Ç–¥. –æ—Ç–∫–ª.': numeric_data.sum(axis=1).std(),
            '–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ –Ω—É–ª–µ–π': (numeric_data == 0).sum(axis=1).mean()
        })
    
    return pd.DataFrame(summary_data)


def interpret_silhouette_score(score: float) -> str:
    """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç —Å–∏–ª—É—ç—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç."""
    if score >= 0.71:
        return "üü¢ –°–∏–ª—å–Ω–æ–µ –∏ —á—ë—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
    elif score >= 0.51:
        return "üü° –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
    elif score >= 0.26:
        return "üü† –°–ª–∞–±–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
    elif score >= 0:
        return "üî¥ –û—á–µ–Ω—å —Å–ª–∞–±–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ"
    else:
        return "‚õî –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"

